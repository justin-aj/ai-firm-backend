# Image Analysis Vector Storage

## Overview

Store image analysis results with embeddings in Milvus vector database for semantic search and retrieval.

**Collection**: `image_analysis_retrieval`

## Quick Start

```python
from clients.image_analyzer_client import ImageAnalyzerClient

# Initialize with embeddings enabled (default)
analyzer = ImageAnalyzerClient()

# Analyze images (embeddings auto-generated)
results = analyzer.describe_images(
    query="machine learning visualization",
    num_images=5
)

# Store in vector database
storage_result = analyzer.store_in_vectordb(
    results=results,
    query="machine learning visualization"
)

print(f"Stored {storage_result['stored']} results")
print(f"IDs: {storage_result['ids']}")
```

## Data Format

### Vector Entry Structure

Following industry best practice for vector storage:

```python
{
  "id": 12345,                    # Auto-generated by Milvus
  "text": "Image title. VLM analysis description...",  # Combined text
  "embedding": [0.02, -0.15, ...],  # 1024-dim BGE-M3 vector
  "metadata": {
    "image_url": "https://...",
    "image_title": "Neural Network Architecture",
    "image_source": "example.com",
    "category": "image_analysis",
    "search_query": "neural network visualization"  # Original query
  }
}
```

### Why This Format?

**Industry Standards:**
1. **Combined Text**: Embed `title + analysis` together (better semantic representation)
2. **Image URL as reference**: Store full URL in metadata (deduplication possible via hash)
3. **Minimal metadata**: Only essential fields (category, source, query context)
4. **Auto IDs**: Let Milvus handle ID generation (collision-free, efficient indexing)

## Embedding Generation

### Automatic (Recommended)

Embeddings are generated automatically during analysis:

```python
# Enable embeddings (default)
analyzer = ImageAnalyzerClient(enable_embeddings=True)

results = analyzer.describe_images("AI concepts", 5)

# Each result has .embedding field
for result in results:
    print(f"Embedding dimension: {len(result.embedding)}")  # 1024
```

### Disable Embeddings

For analysis-only workflows without storage:

```python
# Disable embeddings to save compute
analyzer = ImageAnalyzerClient(enable_embeddings=False)

results = analyzer.describe_images("AI concepts", 5)
# result.embedding will be None
```

## Storage Methods

### store_in_vectordb()

Industry best practice: **Batch insert with individual fallback**

```python
storage_result = analyzer.store_in_vectordb(
    results=results,
    query="optional search query context"
)

# Returns
{
    "stored": 5,
    "failed": 0,
    "ids": [1001, 1002, 1003, 1004, 1005],
    "collection": "image_analysis_retrieval"
}
```

**Behavior:**
1. **Try batch insert** - Fast, efficient (all n images at once)
2. **Fallback to individual** - If batch fails, insert one-by-one
3. **Skip invalid** - Only stores results with embeddings and no errors

### Error Handling

```python
# Some images may fail analysis
results = analyzer.describe_images("test", 10)

# Only successful results with embeddings are stored
storage_result = analyzer.store_in_vectordb(results)

print(f"Analyzed: {len(results)}")
print(f"Stored: {storage_result['stored']}")
print(f"Failed: {storage_result['failed']}")
```

## Collection Schema

### Fields

| Field | Type | Description |
|-------|------|-------------|
| `id` | INT64 | Primary key (auto-generated) |
| `text` | VARCHAR | Combined: "title. analysis" (max 65535 chars) |
| `embedding` | FLOAT_VECTOR | 1024-dim BGE-M3 vector |
| `metadata` | JSON | Image URL, title, source, category, query |

### Index Configuration

**Default**: IVF_FLAT with L2 distance

```python
# Automatically created by store_in_vectordb()
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "L2",
    "params": {"nlist": 128}
}
```

## Retrieval / Search

### Semantic Search (Built-in Method)

The `ImageAnalyzerClient` now has built-in retrieval:

```python
from clients.image_analyzer_client import ImageAnalyzerClient

# Initialize
analyzer = ImageAnalyzerClient()

# Search for similar image analyses
results = analyzer.search_vectordb(
    query="transformer attention mechanism",
    top_k=5
)

# Results are pre-formatted for easy use
for result in results:
    print(f"Similarity: {result['score']:.3f}")
    print(f"Image: {result['image_title']}")
    print(f"Analysis: {result['analysis'][:200]}...")
    print(f"URL: {result['image_url']}\n")
```

### Result Format

```python
{
    "id": 12345,
    "score": 0.856,  # Lower is better for L2 distance
    "image_url": "https://...",
    "image_title": "Neural Network Architecture",
    "image_source": "example.com",
    "analysis": "This diagram shows a feedforward neural network...",
    "search_query": "neural network visualization",  # Original query when stored
    "category": "image_analysis",
    "metadata": {...}  # Full metadata object
}
```

### Advanced Search (Direct Milvus Client)

Find similar image analyses:

```python
from clients.milvus_client import MilvusClient
from clients.embedding_client import EmbeddingClient

# Initialize clients
milvus = MilvusClient(collection_name="image_analysis_retrieval")
embed_client = EmbeddingClient()

# Connect and load collection
milvus.connect()
milvus.collection = milvus.Collection("image_analysis_retrieval")
milvus.collection.load()

# Search for similar images
query = "transformer attention mechanism"
query_embedding = embed_client.generate_embedding(query)

results = milvus.search(
    query_embedding=query_embedding,
    top_k=5,
    output_fields=["text", "metadata"]
)

for result in results:
    print(f"Similarity: {result['score']:.3f}")
    print(f"Image: {result['metadata']['image_title']}")
    print(f"Analysis: {result['text'][:200]}...")
    print(f"URL: {result['metadata']['image_url']}\n")
```

### Filter by Metadata

```python
# Filter by category or search query
results = milvus.collection.query(
    expr='metadata["category"] == "image_analysis"',
    output_fields=["text", "metadata"],
    limit=10
)
```

## Configuration

### GPU Settings

```python
# Single GPU (default)
analyzer = ImageAnalyzerClient(
    gpu_memory_utilization=0.90,  # 90% GPU for VLM
    tensor_parallel_size=1,       # Single GPU
    enable_embeddings=True        # Generate embeddings
)
```

### Milvus Connection

```python
# Custom Milvus server
analyzer = ImageAnalyzerClient(
    milvus_host="192.168.1.100",
    milvus_port="19530"
)

# Collection is always "image_analysis_retrieval"
```

### Environment Variables

```bash
# .env
MILVUS_HOST=localhost
MILVUS_PORT=19530
```

## Complete Workflow Example

### 1. Search, Analyze, and Store

```python
from clients.image_analyzer_client import ImageAnalyzerClient, AnalysisConfig

# Initialize
analyzer = ImageAnalyzerClient(enable_embeddings=True)

# Analyze images with custom config
config = AnalysisConfig(
    num_images=10,
    image_size="xlarge",
    temperature=0.0,
    batch_size=5
)

results = analyzer.search_and_analyze(
    query="neural network architecture diagrams",
    analysis_question="What type of neural network architecture is shown?",
    config=config
)

# Store in vector database
storage_result = analyzer.store_in_vectordb(
    results=results,
    query="neural network architecture diagrams"
)

print(f"Stored {storage_result['stored']} image analyses")
```

### 2. Retrieve Similar Images

```python
from clients.milvus_client import MilvusClient
from clients.embedding_client import EmbeddingClient

# Setup
milvus = MilvusClient(collection_name="image_analysis_retrieval")
embed = EmbeddingClient()

milvus.connect()
milvus.collection = milvus.Collection("image_analysis_retrieval")
milvus.collection.load()

# Search
user_query = "transformer attention mechanism"
query_vec = embed.generate_embedding(user_query)

similar_images = milvus.search(
    query_embedding=query_vec,
    top_k=3
)

# Display results
for img in similar_images:
    print(f"\nScore: {img['score']:.3f}")
    print(f"Title: {img['metadata']['image_title']}")
    print(f"URL: {img['metadata']['image_url']}")
    print(f"Analysis: {img['text'][:150]}...")
```

### 3. RAG Integration

```python
def multimodal_rag_query(user_question: str):
    """RAG with both text and image results"""
    
    # 1. Generate query embedding
    embed = EmbeddingClient()
    query_vec = embed.generate_embedding(user_question)
    
    # 2. Search text collection
    text_results = text_milvus.search(query_vec, top_k=3)
    
    # 3. Search image analysis collection
    image_results = image_milvus.search(query_vec, top_k=2)
    
    # 4. Combine context
    context = []
    
    for r in text_results:
        context.append(f"Text: {r['text']}")
    
    for r in image_results:
        context.append(f"Image ({r['metadata']['image_title']}): {r['text']}")
    
    # 5. Generate answer with VLM
    prompt = f"Context:\n" + "\n\n".join(context)
    prompt += f"\n\nQuestion: {user_question}"
    
    answer = vllm_client.chat_completion(prompt)
    return answer
```

## Performance Optimization

### Batch Processing

```python
# Process 20 images in batches of 5
config = AnalysisConfig(
    num_images=20,
    batch_size=5  # Efficient GPU utilization
)

results = analyzer.search_and_analyze(
    query="data visualization charts",
    analysis_question="What type of chart is shown?",
    config=config
)

# Batch insert into Milvus
storage_result = analyzer.store_in_vectordb(results)
```

### Memory Management

```python
# For large-scale processing
analyzer = ImageAnalyzerClient(
    load_vlm=False,  # Lazy load
    gpu_memory_utilization=0.85,  # Leave headroom
    enable_embeddings=True
)

# VLM loads on first use
results = analyzer.describe_images("AI tech", 100)

# Embeddings use CPU (BGE-M3 on CPU acceptable for throughput)
```

## Troubleshooting

### No Embeddings Generated

```python
# Check if embeddings are enabled
analyzer = ImageAnalyzerClient(enable_embeddings=True)

# Verify in results
for result in results:
    if result.embedding is None:
        print(f"No embedding for: {result.image_title}")
        print(f"Error: {result.error}")
```

### Storage Failed

```python
# Check Milvus connection
try:
    storage_result = analyzer.store_in_vectordb(results)
except Exception as e:
    print(f"Storage error: {e}")
    print("Is Milvus running? docker ps | grep milvus")
```

### Dimension Mismatch

```bash
# Ensure BGE-M3 is used (1024-dim)
# If collection exists with wrong dimension, delete it:

python
>>> from clients.milvus_client import MilvusClient
>>> m = MilvusClient(collection_name="image_analysis_retrieval")
>>> m.connect()
>>> m.delete_collection()
```

## Best Practices

1. **Always enable embeddings** for storage workflows
2. **Batch insert** for efficiency (automatic fallback included)
3. **Store search query** in metadata for context
4. **Use combined text** (title + analysis) for better embeddings
5. **Filter failed results** before storage (automatic)
6. **Index before large searches** (automatic with create_index())
7. **Load collection** before searching (milvus.collection.load())

## Collection Management

### Check Collection Stats

```python
stats = analyzer.milvus_client.get_collection_stats()
print(f"Total vectors: {stats['num_entities']}")
```

### Delete Collection

```python
analyzer.milvus_client.delete_collection()
# Next store_in_vectordb() will recreate it
```

### Manual Collection Setup

```python
from clients.milvus_client import MilvusClient

milvus = MilvusClient(collection_name="image_analysis_retrieval")
milvus.connect()
milvus.create_collection(embedding_dim=1024)
milvus.create_index(index_type="IVF_FLAT", metric_type="L2")
```
